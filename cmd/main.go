package main

import (
	"fmt"
	"math/rand"
	"time"

	"github.com/go-portfolio/go-cnn/cnn"
)

func main() {
	// Инициализация генератора случайных чисел — чтобы веса были случайными при каждом запуске
	rand.Seed(time.Now().UnixNano())

	// -------------------------------------------------------------
	// 1) Генерация случайных весов для сверточного слоя (Conv2D)
	// -------------------------------------------------------------

	// convKernels создаётся как 4-мерный массив:
	// [количество входных каналов][количество фильтров][высота ядра][ширина ядра]
	// Здесь — только 1 входной канал (чёрно-белое изображение), поэтому размер — 1
	convKernels := make([][][][]float64, 1)

	// Создаём набор из 8 случайных свёрточных фильтров 3×3
	kernels := make([][][]float64, 8)
	for k := range kernels {
		kernels[k] = make([][]float64, 3)
		for i := 0; i < 3; i++ {
			kernels[k][i] = make([]float64, 3)
			for j := 0; j < 3; j++ {
				// Случайная инициализация весов маленькими значениями
				kernels[k][i][j] = rand.NormFloat64() * 0.01
			}
		}
	}

	// -------------------------------------------------------------
	// 2) Инициализация полносвязного (Dense) слоя
	// -------------------------------------------------------------
	// После свёртки + ReLU + MaxPool размер тензора становится 1352.
	// Значит, Dense принимает 1352 значений на вход.
	// Выход — 10 чисел (классы MNIST).
	W := make([][]float64, 10) // матрица весов 10×1352
	b := make([]float64, 10)   // 10 смещений (bias)

	// Заполняем веса маленькими случайными значениями
	for o := 0; o < 10; o++ {
		W[o] = make([]float64, 1352)
		for i := 0; i < 1352; i++ {
			W[o][i] = rand.NormFloat64() * 0.01
		}
		// bias по умолчанию = 0
		b[o] = 0
	}

	// Скорость обучения для градиентного спуска
	learningRate := 0.01

	// -------------------------------------------------------------
	// 3) Простой цикл обучения (10 эпох)
	// -------------------------------------------------------------
	for epoch := 0; epoch < 10; epoch++ {

		// Вместо реальных данных MNIST — создаём псевдослучайную картинку 28×28
		input := randomImage28x28()

		// Случайная "метка" (класс) — тоже заглушка, замените на реальные метки
		label := rand.Intn(10)

		// --------------------
		// FORWARD PASS
		// --------------------

		// 3×3 свёртка 8-ю фильтрами
		conv := cnn.Conv2D(input, kernels)

		// ReLU активация
		act := cnn.ReLU(conv)

		// MaxPool 2×2 уменьшает размер карты признаков в 4 раза
		pooled := cnn.MaxPool2x2(act)

		// Превращаем 3D-тензор в 1D-вектор длиной 1352
		flat := cnn.Flatten(pooled)

		// Применяем Dense слой: logits = W*x + b
		logits := cnn.Dense(flat, W, b)

		// Softmax для получения вероятностей классов
		pred := cnn. Softmax(logits)

		// Функция потерь — кросс-энтропия
		loss := cnn.CrossEntropy(pred, label)

		fmt.Printf("Epoch %d  Loss=%f\n", epoch, loss)

		// --------------------
		// BACKPROP (только Dense слой)
		// --------------------
		// Градиент по выходу Softmax + CrossEntropy:
		// gradient = (p - one_hot(label))
		gradLogits := make([]float64, len(pred))
		for i := range pred {
			gradLogits[i] = pred[i] // p_i
		}
		gradLogits[label] -= 1 // p_i - 1 на правильном классе

		// Обновление весов Dense слоя:
		// W = W - lr * grad
		for o := 0; o < 10; o++ {
			for i := 0; i < len(flat); i++ {
				W[o][i] -= learningRate * gradLogits[o] * flat[i]
			}
			b[o] -= learningRate * gradLogits[o]
		}

		// Свёрточные веса не обучаются в этом примере.
		// Если нужно — могу написать полную реализацию backprop для Conv2D.
	}
}

// -------------------------------------------------------------
// Функция генерации случайного изображения 28×28
// -------------------------------------------------------------
func randomImage28x28() [][]float64 {
	img := make([][]float64, 28)
	for i := range img {
		img[i] = make([]float64, 28)
		for j := range img[i] {
			// Значения пикселей 0..1
			img[i][j] = rand.Float64()
		}
	}
	return img
}
